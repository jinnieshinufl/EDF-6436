---
title: "Basic Statistics Review with R"
author: "Jinnie shin | jinnie.shin@coe.ufl.edu"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(gradethis)
library(truncnorm)
library(ggplot2)
tutorial_options(exercise.checker = gradethis::grade_learnr)
knitr::opts_chunk$set(echo = FALSE)
```

## 1. Student GPA Data

Here is a sample dataset we will work with (`my.data`)

```{r data, echo=TRUE, exercise=TRUE}

set.seed(3221)

complement <- function(y, rho, x) {
  if (missing(x)) x <- rnorm(length(y)) # Optional: supply a default if `x` is not given
  y.perp <- residuals(lm(x ~ y))
  rho * sd(y.perp) * y + y.perp * sd(y) * sqrt(1 - rho^2)
}

my.data <- data.frame(id = c(1:250), 
                      gender = sample(c(0, 1), replace=TRUE, size=250),
                      age = sample(c(5, 6,7,8), replace=TRUE, size=250),
                      class = sample(c("A", "B"), replace=TRUE, size=250),
                      teacher = sample(c("Jones", 'Ferrar'), replace = TRUE, size=250), 
                      gpa_pre = rtruncnorm(n=250, a=.92, b=4.86, mean=2.81, sd = 0.68), 
                      creativity = rnorm(250, 80.86, 9.570))
my.data$gpa_post = complement(my.data$gpa_pre, 0.752)

```

Let's take a look at `my.data`. 
```{r data_demo, echo=TRUE, exercise=TRUE, exercise.setup='data'}

```

```{r data_show, echo=TRUE, exercise=TRUE, exercise.setup='data'}
head(my.data) # this will give you an overview of your data head()

dim(my.data) # check the dimension 
```
## 2. Commonly used functions in R

| Commonly used functions in R  | Meaning                                    |
|:-----------------------------:|--------------------------------------------|
|             `abs(x)`            | the absolute value of x                    |
|            `exp(x)`             | exponential of x                           |
|           `log10(x)`            | log of x using base 10                     |
|            `sqrt(x) `           | square root of x                           |
|              `xˆy`              | raises x to the power of y (e.g., xˆ2)     |
|           `cor(x, y)`           | correlation of x and y                     |
|            `cor(x)`             | correlation matrix of x                    |
|            `var(x)`             | covariance matrix of x                     |
|             `sd(x)`             | standard deviation of x                    |
|            `mean(x)`            | mean of x                                  |
|           `median(x)`           | median of x                                |
|            `sum(x)`             | sum of the values in x                     |
|            `max(x)`             | maximum value in x                         |
|            `min(x)`             | minimum value in x                         |
|            `sort(x)`            | sort the values in x in ascending order    |
|     `apply(x, 1, function)`     | performs the function for each row in x    |
|     `apply(x, 2, function)`     | performs the function for each column in x |
|    `ifelse(x == y, yes, no)`    | if x is equal to y, do yes, else do no     |


## 3. Variable  

#### What is a **Variable**?
- Any characteristic of persons or things that is observed to take on different values
- The opposite of a variable is a constant.
- **Observed variables** are directly measured in the study wheras **latent variable** are *not* directly measured. However, we estimate latent variables from the observed variables in our study. 

#### Let's take a look at `my.data`. What are the variables? 

```{r two-plus-two3, exercise=TRUE, exercise.setup ="data"}
str(my.data) # this function str() will provide information regarding the data (variable) type. 

```

```{r two-plus-two33, exercise=TRUE, exercise.setup ="data"}
mode(my.data$age) # in order to look at the specific variable [dataset]$[varaible name]
```

```{r mean_mode, exercise=TRUE, exercise.setup ="data"}
# check the mean of the variable ``gpa_pre`` 

```

```{r mean_mode-solution}
mean(my.data$gpa_pre) 
```

```{r mean_mode-check}
grade_this_code()
```

```{r mean_mode2, exercise=TRUE, exercise.setup ="data"}
# check the standard deviation of the variable ``creativity``

```

```{r mean_mode2-solution}
sd(my.data$creativity)
```

```{r mean_mode2-check}
grade_this_code()
```

## 4. Central Tendency 

- **Mode**: Value in one variable's distribution that occurs most frequently
- **Median**: The score which divides one variable's distribution in half 
- **Mean **: The arithmetic average of one variable's values 
 $$ \bar{X} = \frac{\sum_{i = 1}^{N}x_i}{N} = \frac{x_1 +x_2+...+x_N}{N} $$

```{r hist_mode, exercise=TRUE, echo=FALSE, exercise.setup="data"}
hist(my.data$age) # visualize the distribution of the `age` variable 
```

```{r mode, exercise=TRUE, echo=FALSE, exercise.setup ="data"}
# I defined a function to get mode and called this function `getmode` 


getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

getmode(my.data$age)  # Let's see if this works with the variable `age` 

```

```{r mean, exercise=TRUE, echo=FALSE, exercise.setup ="data"}

# Let's print the mean of the variable `gpa_pre` 
mean(my.data$gpa_pre)

hist(my.data$gpa_pre) # histogram 

```

```{r median, exercise=TRUE, echo=FALSE, exercise.setup ="data"}
# let's take a look at median this time. 
x <- c(2,3,3,3,4,4,5,7,7,7,8,8,10,10,11,12,14,14,14) # first we defined a list 'x'
median(x) # print the median 

```

```{r median2, exercise=TRUE, echo=FALSE, exercise.setup ="data"}
# get median of the variable `age`


```

## 5. Spread
It is often useful in statistics to utilize information about how much people deviate from the man of a variable. Let's talk about the difference in these distributions: **GPA_POST 1 vs. GPA-POST 2**

```{r gpa, echo=FALSE, exercise=TRUE, exercise.setup ="data"}
hist(my.data$gpa_post, xlim=c(1,6), main="Histogram for GPA-Post 1 : Mean=2.81, sd=0.68")
k = rtruncnorm(n=250, a=2.0, b =5.45, mean=2.81, sd= 0.20)
hist(k, xlim=c(1,6), main="Histogram for GPA-Post 2: Mean=2.81, sd=0.20")
```

- Univariate deviation scores: Sums of Squares (SS) 
This indicates the **sum of each individual's squared deviation from the mean**
 $$ SS = \sum_{i = 1}^{N}(x_i - \bar{X})^2 $$
---

**[CLASS EXERCISE]**: Let's compute the Sum of Squares for *gpa_post*!

```{r SS, echo=FALSE, exercise=TRUE, exercise.setup ="data"}
# first we will create a list that subtracts the mean from each data point of GPA-Post
var1 = my.data$gpa_post - mean(my.data$gpa_post)
hist(var1) # Did the distribution shape change? 

# Then, we will create list that saves the sum of squares of the elements in var1 
var2 = var1^2 

# finally, we compute the sum! 
SS = sum(var2) 
print(SS)
```

```{r SS2, echo=FALSE, exercise=TRUE, exercise.setup ="data"}
# Compute the Sum of Squares for 'creativity'

```

```{r SS2-solution}

var1 = my.data$creativity - mean(my.data$creativity)

# Then, we will create list that saves the sum of squares of the elements in var1 
var2 = var1^2 

# finally, we compute the sum! 
SS = sum(var2) 
SS
```

```{r SS2-check}
grade_this_code()
``` 



## 6. Z-scores (Standard Score)

- We can represent a person by the value they were assigned for **a varaible**. 
- We can also represent a person by a value that indicates how **far** she deviates from the mean of a variable.
 $$ \hat{z}_i = \frac{x_i- \bar{X}}{s_X} $$
 
```{r Z_score, echo=FALSE, exercise=TRUE, exercise.setup ="data"}
# standard deviation of the variable *GPA-Post*
sd(my.data$gpa_post)

# mean of the variable *GPA-Post*
mean(my.data$gpa_post)

# What is the z-score of a student who had the gpa-post score of 3.56 ?
######## YOUR ANSWER HERE #############
ANSWER = ( 3.56 - mean(my.data$gpa_post) ) / sd(my.data$gpa_post)
print(paste0('Z score is:', ANSWER))

######################################
```

---

**CLASS EXERCISE **

- Q1: Calculate the mean of item 1 and interpret it with respect to item difficulty 
- Q2: Calculate (1) the variance and (2) standard deviation of the test score variable 
- Q3: Calculate person 4's z-score for the test score variable and interpret it 

| Student | Item 1 | Test score |
|---------|--------|------------|
|    1    |    0   |     66     |
|    2    |    0   |     78     |
|    3    |    0   |     89     |
|    4    |    1   |     72     |

```{r q1, echo=FALSE, exercise=TRUE}
# Q1 
item_score = c(0,0,0,1)
#### your code here ###

#######################
```

```{r q1-solution}
item_score = c(0,0,0,1)
mean(item_score)
``` 

```{r q1-check}
grade_this_code()
``` 

```{r q2, echo=FALSE, exercise=TRUE}
# Q2
test_score = c(66, 78, 89, 72)
#### your code here ###

#######################
```

```{r q2-solution}
test_score = c(66, 78, 89, 72)
var(test_score)
sd(test_score)
```

```{r q2-check}
grade_this_code()
```

```{r q3, echo=FALSE, exercise=TRUE}
# Q3 
test_score = c(66, 78, 89, 72)
#### your code here ###

#######################
```

```{r q3-solution}
test_score = c(66, 78, 89, 72)
(72 - mean(test_score) )/sd(test_score)
```

```{r q3-check}
grade_this_code()
```

## 7. Covariance 
- When two variables are related, we would say that the variables *covary*, meaning that they share some common variance between (Y and X1):
![image](https://sites.google.com/site/modernprogramevaluation/_/rsrc/1468739234431/2-ballentine-venn-diagrams/cov-cor%20eqs.png)
- It is clear that these two variables share some variance. This indicates that they covary in some fashion. For instance, as one variable increases, the other one is expected to change in some manner.
  * What does it mean if **A** increases? 
  * What does it mean if **A** decreases? 
  * What does it mean if **A** is positive value?
  * What does it mean if **A** is negative value? 

- Covariance is the measure of the **joint variability** of two random variables. 
- Remember, we learned how we measure a variability (spread) of one variable (deviation)
- In a sample, we estimate the **covariance** of two continuous variables X and Y as:
$$ \hat{c}_{XY} = \frac{1}{N-1}\sum_{i=1}^{N}(x_i-\bar{X})(y_i-\bar{Y}) $$
--- 
 
### Covariance and Correlation 

- *Covariance* values are difficult to interpret because they are dependent on the scale of variables. 
  * E.G. if X1 range from (1 to 10) and X2 range from (1 to 1000) and we are trying to look at how much (X1 vs Y) and (X2 vs Y) covary .. 
  
- *Correlation* is a standardized version of the covariance to evaluate the two variables' relationship (normalized covariance value). 

$$ \hat{r}_{XY} = \frac{1}{N-1}\sum_{i=1}^{N}(\hat{z}_{X_i}\hat{z}_{Y_i}),\\ \hat{z}_i = \frac{x_i- \bar{X}}{s_X} $$
```{r exercise2, echo=FALSE, exercise=TRUE, exercise.setup ="data"}

# Creating the plot
plot(my.data$gpa_post, my.data$gpa_pre, pch = 19, col = "lightblue")

# Regression line
abline(lm(my.data$gpa_pre ~ my.data$gpa_post), col = "red", lwd = 3)

# Pearson correlation
text(paste("Correlation:", round(cor(my.data$gpa_post, my.data$gpa_pre), 2)), x = 25, y = 95)

```

![](https://spss-tutorials.com/img/pearson-correlations-visualized-as-scatterplots.png){ width=75% }

- The correlation coefficient is bound by -1 and 1
- The correlation coefficient measures the direction and strength of a relationship between two continuous variables
  * Direction (sign): positive vs. negative
  * Strength: between 0 and 1 in magnitude
    * 0 (no relationship)
    * 1 (perfect relationship)

## 8. Explained-variance (R-squared)
- If we square a correlation coefficient, we get an indication of how much variance two variables share.
- Example: 
  $$r_{x,y} = -.24 $$
  $$R^2_{x,y} = .06 $$
- *This indicates that*: 
  * Variables X and Y share **6%** of their variance. 
  * Variable X can explain **6%** of the variance in variable Y. 
  * Variable Y can explain **6%** of the variance in variable X.

--- 

**CLASS EXERCISE **


![Zainudin et al. (2014) - Evaluating the role of Energy Efficiency Label on Customers' Purchasing Behaviour ](https://www.researchgate.net/profile/Siwar-Chamhuri/publication/275530078/figure/tbl1/AS:613863351410722@1523367836730/Pearson-Correlation-Results.png){ width=75% }

-  $Y$ = Green purchasing behaviour

  * Q1: Describe the direction and statistical significance of the correlation coefficients. 
  * Q2: Calculate R2 for each of the correlations and interpret.
  * Q3: Which of the following hypothesis could we reject or accept?
    * H1: consumer knowledge is positively related to behaviour intention.
    * H2: environmental awareness is positively related to behaviour intention.
    * H3: attitudes toward energy efficient products are positively related to behaviour intention.
    * H4: peer pressure will positively affect behavioural intention.
    * H5: energy labelling will positively affect behaviour intention
```{r corr2, echo=FALSE, exercise=TRUE, exercise.setup ="data"}


```

## 9. Regression 
- Purpose: to empirically specify a function that represents the linear relationship between a dependent variable and independent variable(s)

  * E.G., Simple regression = 1 IV
  * E.G., Multiple regression = 2+ IVs

Once specified, the equation can be used to **predict** future “performance”
```{r regression, echo=FALSE}
complement <- function(y, rho, x) {
  if (missing(x)) x <- rnorm(length(y)) # Optional: supply a default if `x` is not given
  y.perp <- residuals(lm(x ~ y))
  rho * sd(y.perp) * y + y.perp * sd(y) * sqrt(1 - rho^2)
}

y <- rnorm(50, sd=10)
x <- 1:50 # Optional
rho <- seq(0, 1, length.out=6) * rep(c(-1,1), 3)
X <- data.frame(z=as.vector(sapply(rho, function(rho) complement(y, rho, x))),
                rho=ordered(rep(signif(rho, 2), each=length(y))),
                y=rep(y, length(rho)))
    
library(ggplot2)
ggplot(X, aes(y,z, group=rho)) + 
  geom_smooth(method="lm", color="Black") + 
  geom_rug(sides="b") + 
  geom_point(aes(fill=rho), alpha=1/2, shape=21) +
  facet_wrap(~ rho, scales="free")
```

R has a built-in function for fitting linear regression models. The function ``lm()`` can be used to fit bivariate and multiple regression models, as well asanalysis of variance, analysis of covariance, and other linear models.

Let's fit a regression line with a DV: ``gpa_post`` and a IV: `gpa_pre`

```{r regression11, echo=FALSE, exercise=TRUE, exercise.setup ="data"}
fit.reg <-lm(gpa_post ~ gpa_pre, my.data)
```

```{r regression2, echo=FALSE, exercise=TRUE, exercise.setup ="regression11"}

summary(fit.reg)

assign("fit.reg", fit.reg, envir=globalenv()) # we will save and run
```

Let's plot this! 
```{r, plot1, echo=FALSE, exercise=TRUE, exercise.setup ="data"}
ggplot(my.data, aes(x = gpa_pre, y = gpa_post)) + 
  geom_point(alpha=0.7) +   
  geom_abline(slope = fit.reg$coefficients[[2]],
              intercept = fit.reg$coefficients[[1]],
              color='red', alpha=0.5)

```

```{r, plot2, echo=TRUE, exercise=TRUE, exercise.setup ="data"}

ggplot(my.data, aes(x = gpa_pre, y = gpa_post)) + 
  geom_point(alpha=0.7) +                           # observed data
  geom_point(aes(x = gpa_pre, y = fit.reg$fitted.values),  # predicted data
             color='red', alpha=0.5) +
  geom_segment(aes(xend = gpa_pre, yend = fit.reg$fitted.values),
               color='red', linetype='dashed', alpha=0.25)

```

```{r letter-a, echo=FALSE}
question("For every one unit increase in X, what is the expected change in Y?",
  answer("0.0315"),
  answer("1.1611"),
  answer("0.7853", correct = TRUE),
  answer("0.4597"),
  answer("0.5655")
)
```

```{r letter-c, echo=FALSE}
question("The test for correlation will lead to the same conclusion as the test for slope.",
  answer("True", correct = TRUE, message="The sign of the slope (i.e. negative or positive) will be the same for the correlation. In other words, both values indicate the direction of the relationship"),
  answer("False")
)
```

```{r letter-d, echo=FALSE}
question("The value of the correlation indicates the strength of the linear relationship. The value of the slope does not.",
  answer("True", correct = TRUE, message="The slope interpretation tells you the change in the response for a one-unit increase in the predictor. Correlation does not have this kind of interpretation."),
  answer("False")
)
```
